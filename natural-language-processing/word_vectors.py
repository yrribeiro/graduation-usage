# -*- coding: utf-8 -*-
"""lista5-pln.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QK9qJHg5reLuhP88DyCxtrweRALsd-A5

**Alunos**: João Pedro Nunes e Yanka Ribeiro
"""

import warnings
import time
import random
import pandas as pd
import gensim
import numpy as np
from bokeh.plotting import figure
from bokeh.palettes import Category20
from bokeh.io import show, output_notebook
from bokeh.resources import INLINE
output_notebook(INLINE)
from gensim.models import KeyedVectors
import gensim.downloader as api
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from sklearn.linear_model import LogisticRegression
from matplotlib import pyplot as plt
from sklearn.metrics import plot_confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
!pip install yellowbrick
from yellowbrick.cluster import KElbowVisualizer
!pip install umap-learn
import umap

warnings.filterwarnings('ignore')

word2vec = api.load('glove-twitter-100')

texts = list(pd.read_csv('https://gist.githubusercontent.com/yrribeiro/0ec3c900b848f0f7bab64bbcc16274e4/raw/d8c268bf1b7b51e217213e40509b4c229c656787/texts.csv',
                      delimiter=',')['text'])
texts

"""# **Questão 1: *O objetivo dessa questão é desenvolver um buscador de documentos.***

# **a)** *escolha e aplique um modelo do tipo word2vec a seus textos, compatível com o idioma de seus textos (inglês ou português).*
"""

query_words = []
splitted_texts = []

for i in texts:
  sample = i.split()
  splitted_texts.append(sample)
  if len(sample) < 2:
    diff_word = sample[0]
  else:
    diff_word = word2vec.doesnt_match(sample)
  
  if len(query_words) < 5 and diff_word not in query_words:
    query_words.append(diff_word)

  print(f'-- FRASE: "{i}"\nPALAVRA DISTOANTE: {diff_word}\n')

"""# **b)** *escolha 5 palavras de consulta que não estejam em nenhum dos textos. Para cada palavra de consulta, encontre as 3 palavras de seu conjunto de textos mais parecidas com cada uma das palavras de consulta e exiba os documentos onde estas palavras aparecem.*"""

# for idx, i in enumerate(query_words):                            #FEITO ANTERIORMENTE
#   query_words[idx] = word2vec.most_similar(positive=i)[2][0]     #PARA EVITAR RESULTADOS DIFERENTES ENTRE EXECUÇÕES DE PROGRAMAS, DECIDIMOS MANTER UMA LISTA FIXA
query_words = ['foward', 'northeast', 'exquisite', 'berry', 'terrific']
print(f'-- Palavras de consulta: {query_words}\n')

# conferindo a ausência das palavras
has_a_query_word = False
for sentence in splitted_texts:
  for query_word in query_words:
    if query_word in sentence:
      print(f'~~ palavra "{query_word}" presente em {sentence}')
      has_a_query_word = True
      break

if not has_a_query_word:
  print('         -- não foram encontradas palavras de consulta entre os documentos! --')

# removendo palavras que não estão no vocabulário do modelo (OOVs)
w2v_vocab = word2vec.wv.vocab
all_tokens = set([token for sentence in splitted_texts for token in sentence])
print('-- total de tokens antes da remoção de OOVs: ', len(all_tokens))
oov = set([token for token in all_tokens if token not in w2v_vocab])
print('-- total de OOVs: ', len(oov))

all_tokens = list(all_tokens - oov)
print('-- tamanho final da lista com todos os tokens: ', len(all_tokens))

no_oov_text = []
for sentence in splitted_texts:
  to_add = []
  for word in sentence:
    if word not in oov:
      to_add.append(word)
  no_oov_text.append(to_add)
del no_oov_text[1545]
del no_oov_text[831]

def search_document(query_word):
  return_sentence = 'none'
  for sentence in splitted_texts:
    if query_word in sentence:
      return_sentence = ' '.join(sentence)
      break
  return return_sentence

for word in query_words:
    print('\n')
    result = word2vec.most_similar(positive=word, topn=3)
    print(f'-- mais similar a "{word}": {result}')
    for i in result:
      print(f'frase do doc: {search_document(i[0])}\n')

"""# **c)** *Seja d um documento da base e w uma palavra de consulta. Implemente o seguinte algoritmo para buscar documentos:*"""

chosen_qw = query_words[1]
print('   -- palavra de consulta escolhida: ', chosen_qw)

"""## 1. Encontre d10(w): a lista com as 10 palavras mais parecidas com w em um certo documento d.

"""

more_ten = [i for i in no_oov_text if len(i) >= 10]
d = more_ten[6]
print(f'-- FRASE: {" ".join(d)}')
all_dist = []
print('\nDa mais similar para a menos...')
for token in d:
  all_dist.append((token, word2vec.distance(chosen_qw, token)))
all_dist.sort(key=lambda x: x[1])

for token, sim_rate in all_dist:
  print(f'   -- distancia de "{token}" para "{chosen_qw}" =  {sim_rate:.2f}')

"""## 2. Para cada documento d, calcule a distância média DM10(w) entre w e as palavras de d10(w)."""

doc_and_distances = []
for sentence in no_oov_text:
  print('\n')
  n = len(sentence)
  dist_sum = 0
  whole_phrase = ' '.join(sentence)
  print(f'-- FRASE: {whole_phrase}')
  for token in sentence:
    dist = word2vec.distance(token, chosen_qw)
    dist_sum += dist
    print(f'   -- distância de "{token}" até "{chosen_qw}" >> {dist:.2f}')

  dm = n and (dist_sum/n)
  print(f'-- DM10({chosen_qw}) = {dm:.2f}')
  doc_and_distances.append((whole_phrase, dm))

"""## 3. Recupere os 3 documentos da base cuja DM10(w) é menor."""

doc_and_distances.sort(key=lambda x:x[1])
del doc_and_distances[0:2] # os dois primeiros documentos são vazios e com DM = 0
print(f'-- palavra de consulta: {chosen_qw}\n\n{doc_and_distances[1:4]}') # doc_and_distances[0] é um documento com um único token, 'wv', não é interesante para a análise

"""# **d)** *aplique o algoritmo para buscar documentos em 5 palavras distintas, e exiba os 3 documentos mais próximos de cada um.*"""

# distinct_words = random.sample(all_tokens, 5) FEITO ANTERIORMENTE
distinct_words = ['comp', 'depicts', 'scattered', 'disturbance', 'reevaluate']
print(f'-- 5 palavras distintas: {distinct_words}')
found_docs = []
for sentence in no_oov_text:
  for word in distinct_words:
    if word in sentence:
      # whole_phrase = 
      print(f'  -- documento com a palavra "{word}": {" ".join(sentence)}')
      found_docs.append((word, sentence))
# found_docs = set(found_docs)

for doc in found_docs:
  all_wm_distances = []
  print(f'-- FRASE: {" ".join(doc[1])}    |     PALAVRA DISTINTA CORRESPONDENTE: {doc[0]}')
  for sentence in no_oov_text:
    result = word2vec.wmdistance(doc[1], sentence)
    all_wm_distances.append((sentence, result))
  all_wm_distances.sort(key=lambda x:x[1])
  print(f'   -- frases mais próximas: {all_wm_distances[1]}\n      {all_wm_distances[2]}\n      {all_wm_distances[3]}') # all_wm_distances[0] será a própria sentença, 
  print('\n')                                                                                                           # que obviamente terá 100% (ou distância 0) de similaridade

"""# **Questão 2: *Aplicando a representação Doc2Vec:***

# **a)** *Resolva a segunda questão da 3a lista com esta nova representação e compare com os resultados obtidos anteriormente.*

**Questão 2**: Elabore um problema de classificação binária de textos coerente com sua base.

**a)** Determine o rótulo dos documentos (separando os documentos em classes bem definidas)

---
**b)** Extraia as representações vetoriais com CountVectorizer e TF-IDF

---
**c)** Treine um classificador baseado em cada uma das duas representações vetoriais e Regressão Logística usando validação cruzada com 70% das amostras selecionadas para treino e 30% para teste. Exiba as matrizes de confusão, métricas de acurácia, precis̃ao, recall e F1 score

---
**d)** Faça o mesmo para o classificador Naive-Bayes

---
**e)** Faça o mesmo para um outro classificador de sua preferência (pesquise na biblioteca Scikit-learn)

---
**f)** Compare os 6 resultados

---
"""

docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(no_oov_text)]
doc_embedding = Doc2Vec(docs, vector_size = 300, window = 10, min_count = 2, workers = 8)

X = [doc_embedding.infer_vector(u) for u in no_oov_text]

targets = pd.read_csv('https://gist.githubusercontent.com/yrribeiro/8d2dfca458e41716d11844a801318f5f/raw/c93ecb29b6c79edb5e940d4e72aa97348850260d/only_target.csv')
y = list(targets['target'][:7388])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
rus = RandomUnderSampler()
X_res, y_res = rus.fit_resample(X_train, y_train)

lr_model = LogisticRegression().fit(X_res, y_res)
y_pred = lr_model.predict(X_test)

plot_confusion_matrix(lr_model, X_test, y_test)
plt.show()

accuracy_lr = accuracy_score(y_test, y_pred)
precision_lr = precision_score(y_test, y_pred)
recall_lr = recall_score(y_test, y_pred)
f1_lr = f1_score(y_test, y_pred)

print(f'-- REGRESSÃO LOGÍSTICA\n   acurária: {accuracy_lr}\n   precisão: {precision_lr}\n   recall: {recall_lr}\n   f1 Score: {f1_lr}\n')

svc_model = SVC().fit(X_res, y_res)
y_pred = svc_model.predict(X_test)

plot_confusion_matrix(svc_model, X_test, y_test)
plt.show()

accuracy_svm = accuracy_score(y_test, y_pred)
precision_svm = precision_score(y_test, y_pred)
recall_svm = recall_score(y_test, y_pred)
f1_svm = f1_score(y_test, y_pred)

print(f'-- SVM\n   acurária: {accuracy_svm}\n   precisão: {precision_svm}\n   recall: {recall_svm}\n   f1 Score: {f1_svm}\n')

lr_results = [accuracy_lr, precision_lr, recall_lr, f1_lr]
svm_results = [accuracy_svm, precision_svm, recall_svm, f1_svm]

data = {
    'Regressão Logística': lr_results,
    'SVM': svm_results,
}

row_names = ['acurácia', 'precisão', 'recall', 'f1']
# pd.set_option('display.max_columns', None)

result = pd.DataFrame(data, index=row_names)

result

"""# **b)** *Resolva a segunda e terceira questão da 4a lista com esta nova representação e compare com os resultados obtidos anteriormente.*"""

X_proj = PCA(n_components=.95).fit_transform(X)
print(X_proj.shape[1])
visualizer = KElbowVisualizer(KMeans(), k=(10, 25), metric='distortion')
visualizer.fit(X_proj)
visualizer.show()

k = 15
kmeans = KMeans(n_clusters=k)
kmeans.fit(X_proj)

start = time.time()
X_tsne = TSNE(n_components=2).fit_transform(X_proj)
exec_time = time.time() - start
print(f'-- {exec_time:.2f} segundos')

def scatter_plot(X, labels):
    palette = Category20[k]
    plot = figure(plot_width=1000, plot_height=500)
    plot.circle(X[:, 0], X[:, 1], size=10, line_color=[palette[l] for l in labels], fill_color=[palette[l] for l in labels], fill_alpha=0.8)
    show(plot)

scatter_plot(X_tsne, kmeans.labels_)

start = time.time()
X_umap = umap.UMAP().fit_transform(X_proj)
exec_time = time.time() - start
print(f'-- {exec_time:.2f} segundos')

scatter_plot(X_umap, kmeans.labels_)